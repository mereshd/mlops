{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "**Machine Learning Operations -- Assignment 2**\n",
        "\n",
        "**11/1/2023**\n",
        "\n",
        "**Danil Meresenschi, Jason Rajan, Ian Vogt, Monicha Zhang**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Handle to Workspace**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# authenticate\n",
        "credential = DefaultAzureCredential()\n",
        "# # Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=\"5405ca59-d0a7-4e18-af16-b7f21ebcb1b0\",\n",
        "    resource_group_name=\"MLOps_group6\",\n",
        "    workspace_name=\"MLOps_Assignment2\",\n",
        ")\n",
        "cpu_cluster = None"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "ml_client",
        "gather": {
          "logged": 1698868616612
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "**Access Registered Data Asset**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import mltable\n",
        "import pandas as pd\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "#ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
        "#data_asset = ml_client.data.get(\"Assignment2_test_data\", version=\"1\")\n",
        "\n",
        "ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
        "data_asset = ml_client.data.get(\"Assignment2_test_data\", version=\"1\")\n",
        "\n",
        "print(data_asset.path)\n",
        "path = {\n",
        "  'folder': data_asset.path\n",
        "}\n",
        "df = pd.read_csv(data_asset.path)\n",
        "#tbl = mltable.from_delimited_files(paths=[path])\n",
        "#df = tbl.to_pandas_dataframe()\n",
        "df"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found the config file in: /config.json\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "azureml://subscriptions/5405ca59-d0a7-4e18-af16-b7f21ebcb1b0/resourcegroups/mlops_group6/workspaces/mlops_assignment2/datastores/workspaceblobstore/paths/UI/2023-10-31_025318_UTC/athletes.csv/\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "        athlete_id             name               region  \\\n0           2554.0        Pj Ablang           South West   \n1           3517.0    Derek Abdella                  NaN   \n2           4691.0              NaN                  NaN   \n3           5164.0      Abo Brandon  Southern California   \n4           5286.0      Bryce Abbey                  NaN   \n...            ...              ...                  ...   \n423001    574489.0       Odo Renata        Latin America   \n423002    585696.0    Lozzie Trevor            Australia   \n423003    608828.0    Marisol Smith           North West   \n423004    628881.0  Pedrini Morgane               Europe   \n423005    631847.0       Eden Kenny  Northern California   \n\n                           team              affiliate  gender   age  height  \\\n0                   Double Edge   Double Edge CrossFit    Male  24.0    70.0   \n1                           NaN                    NaN    Male  42.0    70.0   \n2                           NaN                    NaN     NaN   NaN     NaN   \n3                  LAX CrossFit           LAX CrossFit    Male  40.0    67.0   \n4                           NaN                    NaN    Male  32.0    65.0   \n...                         ...                    ...     ...   ...     ...   \n423001        Team Guaruj√° Inox       CrossFit Guaruja  Female  36.0     NaN   \n423002  FBP CrossFit Games Team           FBP CrossFit  Female  27.0     NaN   \n423003      CrossFit Oak Harbor    CrossFit Oak Harbor  Female  44.0     NaN   \n423004                      NaN            CrossFit 67  Female  20.0    64.0   \n423005                      NaN  Contra Costa CrossFit  Female  26.0     NaN   \n\n        weight   fran  ...  snatch  deadlift  backsq  pullups  \\\n0        166.0    NaN  ...     NaN     400.0   305.0      NaN   \n1        190.0    NaN  ...     NaN       NaN     NaN      NaN   \n2          NaN    NaN  ...     NaN       NaN     NaN      NaN   \n3          NaN  211.0  ...   200.0     375.0   325.0     25.0   \n4        149.0  206.0  ...   150.0       NaN   325.0     50.0   \n...        ...    ...  ...     ...       ...     ...      ...   \n423001     NaN    NaN  ...     NaN       NaN     NaN      NaN   \n423002     NaN    NaN  ...     NaN       NaN     NaN      NaN   \n423003     NaN    NaN  ...     NaN       NaN     NaN      NaN   \n423004    61.0    NaN  ...     NaN      80.0   143.0      NaN   \n423005     NaN    NaN  ...     NaN       NaN     NaN      NaN   \n\n                                                      eat  \\\n0                                                     NaN   \n1                                                     NaN   \n2                                                     NaN   \n3                    I eat 1-3 full cheat meals per week|   \n4       I eat quality foods but don't measure the amount|   \n...                                                   ...   \n423001                                                NaN   \n423002                                                NaN   \n423003                                                NaN   \n423004  I eat quality foods but don't measure the amount|   \n423005                                                NaN   \n\n                                                    train  \\\n0       I workout mostly at a CrossFit Affiliate|I hav...   \n1       I have a coach who determines my programming|I...   \n2                                                     NaN   \n3       I workout mostly at a CrossFit Affiliate|I hav...   \n4       I workout mostly at a CrossFit Affiliate|I inc...   \n...                                                   ...   \n423001                                                NaN   \n423002                                                NaN   \n423003                                                NaN   \n423004          I workout mostly at a CrossFit Affiliate|   \n423005                                                NaN   \n\n                                               background  \\\n0       I played youth or high school level sports|I r...   \n1             I played youth or high school level sports|   \n2                                                     NaN   \n3             I played youth or high school level sports|   \n4                                I played college sports|   \n...                                                   ...   \n423001                                                NaN   \n423002                                                NaN   \n423003                                                NaN   \n423004                                                NaN   \n423005                                                NaN   \n\n                                               experience  \\\n0       I began CrossFit with a coach (e.g. at an affi...   \n1       I began CrossFit with a coach (e.g. at an affi...   \n2                                                     NaN   \n3       I began CrossFit by trying it alone (without a...   \n4       I began CrossFit by trying it alone (without a...   \n...                                                   ...   \n423001                                                NaN   \n423002                                                NaN   \n423003                                                NaN   \n423004  I began CrossFit with a coach (e.g. at an affi...   \n423005                                                NaN   \n\n                                                 schedule       howlong  \n0              I do multiple workouts in a day 2x a week|     4+ years|  \n1              I do multiple workouts in a day 2x a week|     4+ years|  \n2                                                     NaN           NaN  \n3                      I usually only do 1 workout a day|     4+ years|  \n4       I usually only do 1 workout a day|I strictly s...    1-2 years|  \n...                                                   ...           ...  \n423001                                                NaN           NaN  \n423002                                                NaN           NaN  \n423003                                                NaN           NaN  \n423004  I usually only do 1 workout a day|I strictly s...  6-12 months|  \n423005                                                NaN           NaN  \n\n[423006 rows x 27 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>athlete_id</th>\n      <th>name</th>\n      <th>region</th>\n      <th>team</th>\n      <th>affiliate</th>\n      <th>gender</th>\n      <th>age</th>\n      <th>height</th>\n      <th>weight</th>\n      <th>fran</th>\n      <th>...</th>\n      <th>snatch</th>\n      <th>deadlift</th>\n      <th>backsq</th>\n      <th>pullups</th>\n      <th>eat</th>\n      <th>train</th>\n      <th>background</th>\n      <th>experience</th>\n      <th>schedule</th>\n      <th>howlong</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2554.0</td>\n      <td>Pj Ablang</td>\n      <td>South West</td>\n      <td>Double Edge</td>\n      <td>Double Edge CrossFit</td>\n      <td>Male</td>\n      <td>24.0</td>\n      <td>70.0</td>\n      <td>166.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>400.0</td>\n      <td>305.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I workout mostly at a CrossFit Affiliate|I hav...</td>\n      <td>I played youth or high school level sports|I r...</td>\n      <td>I began CrossFit with a coach (e.g. at an affi...</td>\n      <td>I do multiple workouts in a day 2x a week|</td>\n      <td>4+ years|</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3517.0</td>\n      <td>Derek Abdella</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Male</td>\n      <td>42.0</td>\n      <td>70.0</td>\n      <td>190.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I have a coach who determines my programming|I...</td>\n      <td>I played youth or high school level sports|</td>\n      <td>I began CrossFit with a coach (e.g. at an affi...</td>\n      <td>I do multiple workouts in a day 2x a week|</td>\n      <td>4+ years|</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4691.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5164.0</td>\n      <td>Abo Brandon</td>\n      <td>Southern California</td>\n      <td>LAX CrossFit</td>\n      <td>LAX CrossFit</td>\n      <td>Male</td>\n      <td>40.0</td>\n      <td>67.0</td>\n      <td>NaN</td>\n      <td>211.0</td>\n      <td>...</td>\n      <td>200.0</td>\n      <td>375.0</td>\n      <td>325.0</td>\n      <td>25.0</td>\n      <td>I eat 1-3 full cheat meals per week|</td>\n      <td>I workout mostly at a CrossFit Affiliate|I hav...</td>\n      <td>I played youth or high school level sports|</td>\n      <td>I began CrossFit by trying it alone (without a...</td>\n      <td>I usually only do 1 workout a day|</td>\n      <td>4+ years|</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5286.0</td>\n      <td>Bryce Abbey</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Male</td>\n      <td>32.0</td>\n      <td>65.0</td>\n      <td>149.0</td>\n      <td>206.0</td>\n      <td>...</td>\n      <td>150.0</td>\n      <td>NaN</td>\n      <td>325.0</td>\n      <td>50.0</td>\n      <td>I eat quality foods but don't measure the amount|</td>\n      <td>I workout mostly at a CrossFit Affiliate|I inc...</td>\n      <td>I played college sports|</td>\n      <td>I began CrossFit by trying it alone (without a...</td>\n      <td>I usually only do 1 workout a day|I strictly s...</td>\n      <td>1-2 years|</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>423001</th>\n      <td>574489.0</td>\n      <td>Odo Renata</td>\n      <td>Latin America</td>\n      <td>Team Guaruj√° Inox</td>\n      <td>CrossFit Guaruja</td>\n      <td>Female</td>\n      <td>36.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>423002</th>\n      <td>585696.0</td>\n      <td>Lozzie Trevor</td>\n      <td>Australia</td>\n      <td>FBP CrossFit Games Team</td>\n      <td>FBP CrossFit</td>\n      <td>Female</td>\n      <td>27.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>423003</th>\n      <td>608828.0</td>\n      <td>Marisol Smith</td>\n      <td>North West</td>\n      <td>CrossFit Oak Harbor</td>\n      <td>CrossFit Oak Harbor</td>\n      <td>Female</td>\n      <td>44.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>423004</th>\n      <td>628881.0</td>\n      <td>Pedrini Morgane</td>\n      <td>Europe</td>\n      <td>NaN</td>\n      <td>CrossFit 67</td>\n      <td>Female</td>\n      <td>20.0</td>\n      <td>64.0</td>\n      <td>61.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>80.0</td>\n      <td>143.0</td>\n      <td>NaN</td>\n      <td>I eat quality foods but don't measure the amount|</td>\n      <td>I workout mostly at a CrossFit Affiliate|</td>\n      <td>NaN</td>\n      <td>I began CrossFit with a coach (e.g. at an affi...</td>\n      <td>I usually only do 1 workout a day|I strictly s...</td>\n      <td>6-12 months|</td>\n    </tr>\n    <tr>\n      <th>423005</th>\n      <td>631847.0</td>\n      <td>Eden Kenny</td>\n      <td>Northern California</td>\n      <td>NaN</td>\n      <td>Contra Costa CrossFit</td>\n      <td>Female</td>\n      <td>26.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>423006 rows √ó 27 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "update-credit_data",
        "gather": {
          "logged": 1698868617687
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a Compute Resource**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "# Name assigned to the compute cluster\n",
        "cpu_compute_target = \"cpu-cluster\"\n",
        "\n",
        "try:\n",
        "    # let's see if the compute target already exists\n",
        "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
        "    print(\n",
        "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\n",
        "    )\n",
        "\n",
        "except Exception:\n",
        "    print(\"Creating a new cpu compute target...\")\n",
        "\n",
        "    # Let's create the Azure Machine Learning compute object with the intended parameters\n",
        "    # if you run into an out of quota error, change the size to a comparable VM that is available.\n",
        "    # Learn more on https://azure.microsoft.com/en-us/pricing/details/machine-learning/.\n",
        "    cpu_cluster = AmlCompute(\n",
        "        name=cpu_compute_target,\n",
        "        # Azure Machine Learning Compute is the on-demand VM service\n",
        "        type=\"amlcompute\",\n",
        "        # VM Family\n",
        "        size=\"STANDARD_DS3_V2\",\n",
        "        # Minimum running nodes when there is no job running\n",
        "        min_instances=0,\n",
        "        # Nodes in cluster\n",
        "        max_instances=4,\n",
        "        # How many seconds will the node running after the job termination\n",
        "        idle_time_before_scale_down=180,\n",
        "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
        "        tier=\"Dedicated\",\n",
        "    )\n",
        "    print(\n",
        "        f\"AMLCompute with name {cpu_cluster.name} will be created, with compute size {cpu_cluster.size}\"\n",
        "    )\n",
        "    # Now, we pass the object to MLClient's create_or_update method\n",
        "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "You already have a cluster named cpu-cluster, we'll reuse it as is.\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "name": "cpu_cluster",
        "gather": {
          "logged": 1698868618646
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Environment and Dependencies**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dependencies_dir = \"./dependencies\"\n",
        "os.makedirs(dependencies_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "name": "dependencies_dir",
        "gather": {
          "logged": 1698868619309
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Can be updated as we go\n",
        "#%%writefile {dependencies_dir}/conda.yaml\n",
        "#name: model-env\n",
        "#channels:\n",
        "#  - conda-forge\n",
        "#dependencies:\n",
        "#  - python=3.8\n",
        "#  - numpy=1.21.2\n",
        "#  - pip=21.2.4\n",
        "#  - scikit-learn=0.24.2\n",
        "#  - scipy=1.7.1\n",
        "#  - 'pandas>=1.1,<1.2'\n",
        "#  - pip:\n",
        "#    - inference-schema[numpy-support]==1.3.0\n",
        "#    - xlrd==2.0.1\n",
        "#    - mlflow==2.4.1\n",
        "#    - azureml-mlflow==1.51.0\n",
        "#    - codecarbon==2.3.1"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "conda.yaml",
        "gather": {
          "logged": 1698868620199
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "custom_env_name = \"aml-scikit-learn\"\n",
        "\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for MLOps Assignment 2 pipeline\",\n",
        "    tags={\"scikit-learn\": \"0.24.2\"},\n",
        "    conda_file=os.path.join(dependencies_dir, \"conda.yaml\"),\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "    version=\"0.2.0\",\n",
        ")\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Environment with name aml-scikit-learn is registered to workspace, the environment version is 0.2.0\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "custom_env_name",
        "gather": {
          "logged": 1698868621106
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Data Prep Component**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_prep_src_dir = \"./components/data_prep\"\n",
        "os.makedirs(data_prep_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "data_prep_src_dir",
        "gather": {
          "logged": 1698868621872
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {data_prep_src_dir}/data_prep.py\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import logging\n",
        "import mlflow\n",
        "\n",
        "def select_first_file(path):\n",
        "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
        "    Args:\n",
        "        path (str): path to directory or file to choose\n",
        "    Returns:\n",
        "        str: full path of selected file\n",
        "    \"\"\"\n",
        "    files = os.listdir(path)\n",
        "    return os.path.join(path, files[0])\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.2)\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
        "\n",
        "    print(\"input data:\", args.data)\n",
        "    \n",
        "    #df = pd.read_csv(args.data, header=1, index_col=0, engine='python', encoding='utf-8')\n",
        "    df = pd.read_csv(args.data)\n",
        "    # Retain only the first 423006 rows and 27 columns (original dataset)\n",
        "    #data = df.iloc[:423006, :27]\n",
        "    data = df\n",
        "    # Remove not relevant columns\n",
        "    data = data.dropna(subset=['region','age','weight','height','howlong','gender','eat', \\\n",
        "                           'train','background','experience','schedule', \\\n",
        "                           'deadlift','candj','snatch','backsq'])\n",
        "    data = data.drop(columns=['affiliate','team','name','athlete_id','fran','helen','grace',\\\n",
        "                            'filthy50','fgonebad','run400','run5k','pullups','train'])\n",
        "\n",
        "    data = data[data['weight'] < 1500]\n",
        "    data = data[data['gender'] != '--']\n",
        "    data = data[data['age'] >= 18]\n",
        "    data = data[(data['height'] < 96) & (data['height'] > 48)]\n",
        "\n",
        "    data = data[(data['deadlift'] > 0) & (data['deadlift'] <= 1105)|((data['gender'] == 'Female') \\\n",
        "                & (data['deadlift'] <= 636))]\n",
        "\n",
        "    data = data[(data['candj'] > 0) & (data['candj'] <= 395)]\n",
        "    data = data[(data['snatch'] > 0) & (data['snatch'] <= 496)]\n",
        "    data = data[(data['backsq'] > 0) & (data['backsq'] <= 1069)]\n",
        "\n",
        "    #Dummy encode eat column\n",
        "    unique_outcomes = set()\n",
        "    for response in data['eat']:\n",
        "        if '|' in response:\n",
        "            outcomes = [outcome.strip() for outcome in response.split('|')]\n",
        "            unique_outcomes.update(outcomes)\n",
        "        else:\n",
        "            unique_outcomes.add(response)\n",
        "\n",
        "    #Create columns for each unique outcome and dummy encode\n",
        "    for outcome in unique_outcomes:\n",
        "        column_name = f\"eat_{outcome}\"\n",
        "        data[column_name] = data['eat'].apply(lambda x: 1 if str(outcome) in str(x) else 0)\n",
        "\n",
        "    #Drop the original 'eat' column if needed\n",
        "    data = data.drop(columns=['eat','eat_'])\n",
        "\n",
        "    #binary encode background column\n",
        "    unique_outcomes = set()\n",
        "    for response in data['background']:\n",
        "        if '|' in response:\n",
        "            outcomes = [outcome.strip() for outcome in response.split('|')]\n",
        "            unique_outcomes.update(outcomes)\n",
        "        else:\n",
        "            unique_outcomes.add(response)\n",
        "\n",
        "    #Create columns for each unique outcome and dummy encode\n",
        "    for outcome in unique_outcomes:\n",
        "        column_name = f\"background_{outcome}\"\n",
        "        data[column_name] = data['background'].apply(lambda x: 1 if str(outcome) in str(x) else 0)\n",
        "\n",
        "    #Drop the original 'background' column if needed\n",
        "    data = data.drop(columns=['background','background_'])\n",
        "\n",
        "    #binary encode experience column\n",
        "    unique_outcomes = set()\n",
        "    for response in data['experience']:\n",
        "        if '|' in response:\n",
        "            outcomes = [outcome.strip() for outcome in response.split('|')]\n",
        "            unique_outcomes.update(outcomes)\n",
        "        else:\n",
        "            unique_outcomes.add(response)\n",
        "\n",
        "    #Create columns for each unique outcome and dummy encode\n",
        "    for outcome in unique_outcomes:\n",
        "        column_name = f\"experience_{outcome}\"\n",
        "        data[column_name] = data['experience'].apply(lambda x: 1 if str(outcome) in str(x) else 0)\n",
        "\n",
        "    #Drop the original 'background' column if needed\n",
        "    data = data.drop(columns=['experience','experience_'])\n",
        "\n",
        "    #binary encode schedule column\n",
        "    unique_outcomes = set()\n",
        "    for response in data['schedule']:\n",
        "        if '|' in response:\n",
        "            outcomes = [outcome.strip() for outcome in response.split('|')]\n",
        "            unique_outcomes.update(outcomes)\n",
        "        else:\n",
        "            unique_outcomes.add(response)\n",
        "\n",
        "    #Create columns for each unique outcome and dummy encode\n",
        "    for outcome in unique_outcomes:\n",
        "        column_name = f\"schedule_{outcome}\"\n",
        "        data[column_name] = data['schedule'].apply(lambda x: 1 if str(outcome) in str(x) else 0)\n",
        "\n",
        "    #Drop the original 'background' column if needed\n",
        "    data = data.drop(columns=['schedule','schedule_'])\n",
        "\n",
        "    #binary encode howlong column\n",
        "    unique_outcomes = set()\n",
        "    for response in data['howlong']:\n",
        "        if '|' in response:\n",
        "            outcomes = [outcome.strip() for outcome in response.split('|')]\n",
        "            unique_outcomes.update(outcomes)\n",
        "        else:\n",
        "            unique_outcomes.add(response)\n",
        "\n",
        "    #Create columns for each unique outcome and dummy encode\n",
        "    for outcome in unique_outcomes:\n",
        "        column_name = f\"howlong_{outcome}\"\n",
        "        data[column_name] = data['howlong'].apply(lambda x: 1 if str(outcome) in str(x) else 0)\n",
        "\n",
        "    #Drop the original 'background' column if needed\n",
        "    data = data.drop(columns=['howlong','howlong_'])\n",
        "\n",
        "    # Creating a region dictionary for maping\n",
        "    region_mapping = {\n",
        "        # Region 1 Group\n",
        "        'Africa': 'Region_1',\n",
        "        'Asia': 'Region_1',\n",
        "        'Australia': 'Region_1',\n",
        "        'Latin America': 'Region_1',\n",
        "        \n",
        "        # Region 2 Group\n",
        "        'Canada East': 'Region_2',\n",
        "        'Canada West': 'Region_2',\n",
        "\n",
        "        # Region 3 Group\n",
        "        'Northern California': 'Region_3',\n",
        "        'Southern California': 'Region_3',\n",
        "        \n",
        "        # Region 4 Group\n",
        "        'Central East': 'Region_4',\n",
        "\n",
        "        # Region 5 Group\n",
        "        'Mid Atlantic': 'Region_5',\n",
        "\n",
        "        # Region 6 Group\n",
        "        'North Central': 'Region_6',\n",
        "        'South Central': 'Region_6',\n",
        "\n",
        "        # Region 7 Group\n",
        "        'North East': 'Region_7',\n",
        "        'North West': 'Region_7',\n",
        "\n",
        "        # Region 8 Group  \n",
        "        'South East': 'Region_8',\n",
        "        'South West': 'Region_8',\n",
        "        \n",
        "        # Region 9 Group\n",
        "        'Europe': 'Region_9'\n",
        "    }\n",
        "\n",
        "    # Apply the mapping\n",
        "    data['region'] = data['region'].map(region_mapping)\n",
        "\n",
        "    #encode region and gender\n",
        "    le=LabelEncoder()\n",
        "    data[\"le_region\"] = le.fit_transform(data['region'])\n",
        "    data[\"le_gender\"] = le.fit_transform(data['gender'])\n",
        "\n",
        "    #Lets get the crosswalk for region and gender\n",
        "    data.groupby(['region', 'le_region']).size().reset_index(name='counts')\n",
        "\n",
        "    data.groupby(['gender', 'le_gender']).size().reset_index(name='counts')\n",
        "\n",
        "    #drop unencoded region/gender columns\n",
        "    data = data.drop(columns=['region','gender'])\n",
        "    #calculate our total_lift target value using clean and jerk, snatch,deadlift and backsquat values\n",
        "    data['total_lift'] = data['candj'] + data['snatch'] + data['deadlift'] + data['backsq']\n",
        "\n",
        "    data = data[['total_lift',\n",
        "    'age',\n",
        "    'height',\n",
        "    'weight',\n",
        "    'eat_I eat 1-3 full cheat meals per week',\n",
        "    \"eat_I eat quality foods but don't measure the amount\",\n",
        "    'eat_I eat strict Paleo',\n",
        "    'eat_Decline to answer',\n",
        "    'eat_I eat whatever is convenient',\n",
        "    'eat_I weigh and measure my food',\n",
        "    'background_I played college sports',\n",
        "    'background_I played youth or high school level sports',\n",
        "    'background_I have no athletic background besides CrossFit',\n",
        "    'background_Decline to answer',\n",
        "    'background_I regularly play recreational sports',\n",
        "    'background_I played professional sports',\n",
        "    'experience_I train other people',\n",
        "    'experience_I have had a life changing experience due to CrossFit',\n",
        "    'experience_I began CrossFit by trying it alone (without a coach)',\n",
        "    'experience_I have completed the CrossFit Level 1 certificate course',\n",
        "    'experience_Decline to answer',\n",
        "    'experience_I began CrossFit with a coach (e.g. at an affiliate)',\n",
        "    'experience_I have attended one or more specialty courses',\n",
        "    'schedule_I do multiple workouts in a day 1x a week',\n",
        "    'schedule_I do multiple workouts in a day 3+ times a week',\n",
        "    'schedule_I do multiple workouts in a day 2x a week',\n",
        "    'schedule_I typically rest fewer than 4 days per month',\n",
        "    'schedule_I strictly schedule my rest days',\n",
        "    'schedule_I typically rest 4 or more days per month',\n",
        "    'schedule_Decline to answer',\n",
        "    'schedule_I usually only do 1 workout a day',\n",
        "    'howlong_4+ years',\n",
        "    'howlong_Less than 6 months',\n",
        "    'howlong_6-12 months',\n",
        "    'howlong_2-4 years',\n",
        "    'howlong_Decline to answer',\n",
        "    'howlong_1-2 years',\n",
        "    'le_region',\n",
        "    'le_gender'\n",
        "    ]]\n",
        "\n",
        "    train_df, test_df = train_test_split(data,test_size=0.2,random_state=42)\n",
        "\n",
        "    # output paths are mounted as folder, therefore, we are adding a filename to the path\n",
        "    train_df.to_csv(os.path.join(args.train_data, \"data.csv\"), index=False)\n",
        "\n",
        "    test_df.to_csv(os.path.join(args.test_data, \"data.csv\"), index=False)\n",
        "    return\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/data_prep/data_prep.py\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "def-main"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "data_prep_component = command(\n",
        "    name=\"data_prep\",\n",
        "    display_name=\"Data preparation for training\",\n",
        "    description=\"reads a .xl input, split the input to train and test\",\n",
        "    inputs={\n",
        "        \"data\": Input(type=\"uri_folder\"),\n",
        "        \"test_train_ratio\": Input(type=\"number\"),\n",
        "    },\n",
        "    outputs=dict(\n",
        "        train_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "        test_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "    ),\n",
        "    # The source folder of the component\n",
        "    code=data_prep_src_dir,\n",
        "    command=\"\"\"python data_prep.py \\\n",
        "            --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} \\\n",
        "            --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}} \\\n",
        "            \"\"\",\n",
        "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "name": "data_prep_component",
        "gather": {
          "logged": 1698868623536
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we register the component to the workspace\n",
        "data_prep_component = ml_client.create_or_update(data_prep_component.component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {data_prep_component.name} with Version {data_prep_component.version} is registered\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r\u001b[32mUploading data_prep (0.01 MBs):   0%|          | 0/9618 [00:00<?, ?it/s]\r\u001b[32mUploading data_prep (0.01 MBs): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9618/9618 [00:00<00:00, 135281.98it/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component data_prep with Version 2023-11-01-19-56-59-5969920 is registered\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1698868624218
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a versioning component**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create another component to version the data to v2\n",
        "# This is similar to our data_prep component but also includes\n",
        "# the transformations and feature engineering applied from assignment 1\n",
        "\n",
        "import os\n",
        "\n",
        "data_version_src_dir = \"./components/data_version\"\n",
        "os.makedirs(data_version_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1698868625030
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {data_version_src_dir}/data_version.py\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import logging\n",
        "import mlflow\n",
        "\n",
        "def select_first_file(path):\n",
        "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
        "    Args:\n",
        "        path (str): path to directory or file to choose\n",
        "    Returns:\n",
        "        str: full path of selected file\n",
        "    \"\"\"\n",
        "    files = os.listdir(path)\n",
        "    return os.path.join(path, files[0])\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.2)\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
        "\n",
        "    print(\"input data:\", args.data)\n",
        "    \n",
        "    #df = pd.read_csv(args.data, header=1, index_col=0, engine='python', encoding='utf-8')\n",
        "    df = pd.read_csv(args.data)\n",
        "    # Retain only the first 423006 rows and 27 columns (original dataset)\n",
        "    #data = df.iloc[:423006, :27]\n",
        "    data = df\n",
        "    # Remove not relevant columns\n",
        "    data = data.dropna(subset=['region','age','weight','height','howlong','gender','eat', \\\n",
        "                           'train','background','experience','schedule', \\\n",
        "                           'deadlift','candj','snatch','backsq'])\n",
        "    data = data.drop(columns=['affiliate','team','name','athlete_id','fran','helen','grace',\\\n",
        "                            'filthy50','fgonebad','run400','run5k','pullups','train'])\n",
        "\n",
        "    data = data[data['weight'] < 1500]\n",
        "    data = data[data['gender'] != '--']\n",
        "    data = data[data['age'] >= 18]\n",
        "    data = data[(data['height'] < 96) & (data['height'] > 48)]\n",
        "\n",
        "    data = data[(data['deadlift'] > 0) & (data['deadlift'] <= 1105)|((data['gender'] == 'Female') \\\n",
        "                & (data['deadlift'] <= 636))]\n",
        "\n",
        "    data = data[(data['candj'] > 0) & (data['candj'] <= 395)]\n",
        "    data = data[(data['snatch'] > 0) & (data['snatch'] <= 496)]\n",
        "    data = data[(data['backsq'] > 0) & (data['backsq'] <= 1069)]\n",
        "\n",
        "    #Dummy encode eat column\n",
        "    unique_outcomes = set()\n",
        "    for response in data['eat']:\n",
        "        if '|' in response:\n",
        "            outcomes = [outcome.strip() for outcome in response.split('|')]\n",
        "            unique_outcomes.update(outcomes)\n",
        "        else:\n",
        "            unique_outcomes.add(response)\n",
        "\n",
        "    #Create columns for each unique outcome and dummy encode\n",
        "    for outcome in unique_outcomes:\n",
        "        column_name = f\"eat_{outcome}\"\n",
        "        data[column_name] = data['eat'].apply(lambda x: 1 if str(outcome) in str(x) else 0)\n",
        "\n",
        "    #Drop the original 'eat' column if needed\n",
        "    data = data.drop(columns=['eat','eat_'])\n",
        "\n",
        "    #binary encode background column\n",
        "    unique_outcomes = set()\n",
        "    for response in data['background']:\n",
        "        if '|' in response:\n",
        "            outcomes = [outcome.strip() for outcome in response.split('|')]\n",
        "            unique_outcomes.update(outcomes)\n",
        "        else:\n",
        "            unique_outcomes.add(response)\n",
        "\n",
        "    #Create columns for each unique outcome and dummy encode\n",
        "    for outcome in unique_outcomes:\n",
        "        column_name = f\"background_{outcome}\"\n",
        "        data[column_name] = data['background'].apply(lambda x: 1 if str(outcome) in str(x) else 0)\n",
        "\n",
        "    #Drop the original 'background' column if needed\n",
        "    data = data.drop(columns=['background','background_'])\n",
        "\n",
        "    #binary encode experience column\n",
        "    unique_outcomes = set()\n",
        "    for response in data['experience']:\n",
        "        if '|' in response:\n",
        "            outcomes = [outcome.strip() for outcome in response.split('|')]\n",
        "            unique_outcomes.update(outcomes)\n",
        "        else:\n",
        "            unique_outcomes.add(response)\n",
        "\n",
        "    #Create columns for each unique outcome and dummy encode\n",
        "    for outcome in unique_outcomes:\n",
        "        column_name = f\"experience_{outcome}\"\n",
        "        data[column_name] = data['experience'].apply(lambda x: 1 if str(outcome) in str(x) else 0)\n",
        "\n",
        "    #Drop the original 'background' column if needed\n",
        "    data = data.drop(columns=['experience','experience_'])\n",
        "\n",
        "    #binary encode schedule column\n",
        "    unique_outcomes = set()\n",
        "    for response in data['schedule']:\n",
        "        if '|' in response:\n",
        "            outcomes = [outcome.strip() for outcome in response.split('|')]\n",
        "            unique_outcomes.update(outcomes)\n",
        "        else:\n",
        "            unique_outcomes.add(response)\n",
        "\n",
        "    #Create columns for each unique outcome and dummy encode\n",
        "    for outcome in unique_outcomes:\n",
        "        column_name = f\"schedule_{outcome}\"\n",
        "        data[column_name] = data['schedule'].apply(lambda x: 1 if str(outcome) in str(x) else 0)\n",
        "\n",
        "    #Drop the original 'background' column if needed\n",
        "    data = data.drop(columns=['schedule','schedule_'])\n",
        "\n",
        "    #binary encode howlong column\n",
        "    unique_outcomes = set()\n",
        "    for response in data['howlong']:\n",
        "        if '|' in response:\n",
        "            outcomes = [outcome.strip() for outcome in response.split('|')]\n",
        "            unique_outcomes.update(outcomes)\n",
        "        else:\n",
        "            unique_outcomes.add(response)\n",
        "\n",
        "    #Create columns for each unique outcome and dummy encode\n",
        "    for outcome in unique_outcomes:\n",
        "        column_name = f\"howlong_{outcome}\"\n",
        "        data[column_name] = data['howlong'].apply(lambda x: 1 if str(outcome) in str(x) else 0)\n",
        "\n",
        "    #Drop the original 'background' column if needed\n",
        "    data = data.drop(columns=['howlong','howlong_'])\n",
        "\n",
        "    # Creating a region dictionary for maping\n",
        "    region_mapping = {\n",
        "        # Region 1 Group\n",
        "        'Africa': 'Region_1',\n",
        "        'Asia': 'Region_1',\n",
        "        'Australia': 'Region_1',\n",
        "        'Latin America': 'Region_1',\n",
        "        \n",
        "        # Region 2 Group\n",
        "        'Canada East': 'Region_2',\n",
        "        'Canada West': 'Region_2',\n",
        "\n",
        "        # Region 3 Group\n",
        "        'Northern California': 'Region_3',\n",
        "        'Southern California': 'Region_3',\n",
        "        \n",
        "        # Region 4 Group\n",
        "        'Central East': 'Region_4',\n",
        "\n",
        "        # Region 5 Group\n",
        "        'Mid Atlantic': 'Region_5',\n",
        "\n",
        "        # Region 6 Group\n",
        "        'North Central': 'Region_6',\n",
        "        'South Central': 'Region_6',\n",
        "\n",
        "        # Region 7 Group\n",
        "        'North East': 'Region_7',\n",
        "        'North West': 'Region_7',\n",
        "\n",
        "        # Region 8 Group  \n",
        "        'South East': 'Region_8',\n",
        "        'South West': 'Region_8',\n",
        "        \n",
        "        # Region 9 Group\n",
        "        'Europe': 'Region_9'\n",
        "    }\n",
        "\n",
        "    # Apply the mapping\n",
        "    data['region'] = data['region'].map(region_mapping)\n",
        "\n",
        "    #encode region and gender\n",
        "    le=LabelEncoder()\n",
        "    data[\"le_region\"] = le.fit_transform(data['region'])\n",
        "    data[\"le_gender\"] = le.fit_transform(data['gender'])\n",
        "\n",
        "    #Lets get the crosswalk for region and gender\n",
        "    data.groupby(['region', 'le_region']).size().reset_index(name='counts')\n",
        "\n",
        "    data.groupby(['gender', 'le_gender']).size().reset_index(name='counts')\n",
        "\n",
        "    #drop unencoded region/gender columns\n",
        "    data = data.drop(columns=['region','gender'])\n",
        "    #calculate our total_lift target value using clean and jerk, snatch,deadlift and backsquat values\n",
        "    data['total_lift'] = data['candj'] + data['snatch'] + data['deadlift'] + data['backsq']\n",
        "    \n",
        "    # Apply some transformations and feature engineering\n",
        "    # Adding a new domain-specific feature BMI, must round then convert inches to meters and pounds to killograms.\n",
        "    data['BMI'] = round((data['weight'] * 0.453592) / (data['height']*0.0254)**2,1)\n",
        "\n",
        "    # Binning age, height, and weight, region, BMI index\n",
        "    age_bins = [0, 20, 30, 40, 50, 60, 100]\n",
        "    age_labels = ['0-20', '20-30', '30-40', '40-50', '50-60', '60+']  # Adjust labels as needed\n",
        "\n",
        "    # height_bins = [0, 60, 65, 70, 75, 80, 100]\n",
        "    # height_labels = ['0-60', '60-65', '65-70', '70-75', '75-80', '80+']\n",
        "\n",
        "    # weight_bins = [0, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325, 350, 750]\n",
        "    # weight_labels = ['0-100', '100-125', '125-150', '150-175', '175-200', '200-225', '225-250', '250-275', '275-300',\n",
        "    #                 '300-325', '325-350', '350+']\n",
        "\n",
        "    region_bins = [0,2,5,9]\n",
        "    region_labels = ['0-2','2-5','6+']\n",
        "\n",
        "    BMI_bins = [0,16,20,25,30,35,39,70]\n",
        "    BMI_labels = ['0-16','16-20','20-25','25-30','30-35','35-39','39+']\n",
        "\n",
        "    data['binned_age'] = pd.cut(data['age'], age_bins, labels=age_labels)\n",
        "    #data['binned_height'] = pd.cut(data['height'], height_bins, labels=height_labels)\n",
        "    #data['binned_weight'] = pd.cut(data['weight'], weight_bins, labels=weight_labels)\n",
        "    data['binned_region'] = pd.cut(data['le_region'], region_bins, labels=region_labels)\n",
        "    data['binned_BMI'] = pd.cut(data['BMI'], BMI_bins, labels=BMI_labels)\n",
        "\n",
        "    #encode new binned variables\n",
        "    le=LabelEncoder()\n",
        "    data[\"le_binned_age\"] = le.fit_transform(data['binned_age'])\n",
        "    #data[\"le_binned_height\"] = le.fit_transform(data['binned_height'])\n",
        "    #data[\"le_binned_weight\"] = le.fit_transform(data['binned_weight'])\n",
        "    data[\"le_binned_region\"] = le.fit_transform(data['binned_region'])\n",
        "    data[\"le_binned_BMI\"] = le.fit_transform(data['binned_BMI'])\n",
        "\n",
        "    # Multiply height and weight together to capture some interaction effect\n",
        "    data[\"height_times_weight\"] = data[\"height\"] * data[\"weight\"]\n",
        "\n",
        "    # Square age\n",
        "    data[\"age_squared\"] = data[\"age\"] * data[\"age\"]\n",
        "\n",
        "    # Square weight\n",
        "    data[\"weight_squared\"] = data[\"weight\"] * data[\"weight\"]\n",
        "\n",
        "    # Square BMI\n",
        "    data['BMI_squared'] = data['BMI'] * data['BMI'] \n",
        "\n",
        "    # Retain only certain columns for train/test, these are the same used in assignment 1\n",
        "    data = data[['total_lift',\n",
        "    'age_squared',\n",
        "    'BMI_squared',\n",
        "    'eat_I eat 1-3 full cheat meals per week',\n",
        "    \"eat_I eat quality foods but don't measure the amount\",\n",
        "    'eat_I eat strict Paleo',\n",
        "    'eat_Decline to answer',\n",
        "    'eat_I eat whatever is convenient',\n",
        "    'eat_I weigh and measure my food',\n",
        "    'background_I played college sports',\n",
        "    'background_I played youth or high school level sports',\n",
        "    'background_I have no athletic background besides CrossFit',\n",
        "    'background_Decline to answer',\n",
        "    'background_I regularly play recreational sports',\n",
        "    'background_I played professional sports',\n",
        "    'experience_I train other people',\n",
        "    'experience_I have had a life changing experience due to CrossFit',\n",
        "    'experience_I began CrossFit by trying it alone (without a coach)',\n",
        "    'experience_I have completed the CrossFit Level 1 certificate course',\n",
        "    'experience_Decline to answer',\n",
        "    'experience_I began CrossFit with a coach (e.g. at an affiliate)',\n",
        "    'experience_I have attended one or more specialty courses',\n",
        "    'schedule_I do multiple workouts in a day 1x a week',\n",
        "    'schedule_I do multiple workouts in a day 3+ times a week',\n",
        "    'schedule_I do multiple workouts in a day 2x a week',\n",
        "    'schedule_I typically rest fewer than 4 days per month',\n",
        "    'schedule_I strictly schedule my rest days',\n",
        "    'schedule_I typically rest 4 or more days per month',\n",
        "    'schedule_Decline to answer',\n",
        "    'schedule_I usually only do 1 workout a day',\n",
        "    'howlong_4+ years',\n",
        "    'howlong_Less than 6 months',\n",
        "    'howlong_6-12 months',\n",
        "    'howlong_2-4 years',\n",
        "    'howlong_Decline to answer',\n",
        "    'howlong_1-2 years',\n",
        "    'le_binned_region',\n",
        "    'le_gender'\n",
        "    ]]\n",
        "\n",
        "    train_df, test_df = train_test_split(data,test_size=0.2,random_state=42)\n",
        "\n",
        "    # output paths are mounted as folder, therefore, we are adding a filename to the path\n",
        "    train_df.to_csv(os.path.join(args.train_data, \"data.csv\"), index=False)\n",
        "\n",
        "    test_df.to_csv(os.path.join(args.test_data, \"data.csv\"), index=False)\n",
        "    return\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/data_version/data_version.py\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "data_version_component = command(\n",
        "    name=\"data_version\",\n",
        "    display_name=\"Data versioning to v2 for training\",\n",
        "    description=\"reads a .xl input, split the input to train and test\",\n",
        "    inputs={\n",
        "        \"data\": Input(type=\"uri_folder\"),\n",
        "        \"test_train_ratio\": Input(type=\"number\"),\n",
        "    },\n",
        "    outputs=dict(\n",
        "        train_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "        test_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "    ),\n",
        "    # The source folder of the component\n",
        "    code=data_version_src_dir,\n",
        "    command=\"\"\"python data_version.py \\\n",
        "            --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} \\\n",
        "            --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}} \\\n",
        "            \"\"\",\n",
        "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1698868626410
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we register the component to the workspace\n",
        "data_version_component = ml_client.create_or_update(data_version_component.component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {data_version_component.name} with Version {data_version_component.version} is registered\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component data_version with Version 2023-11-01-19-57-01-3445267 is registered\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1698868627140
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Model Training Components**\n",
        "\n",
        "**First One Will Mimic the Regression Model from Assignment 1**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "train_src_dir = \"./components/train\"\n",
        "os.makedirs(train_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "train_src_dir",
        "gather": {
          "logged": 1698868627846
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {train_src_dir}/train.py\n",
        "import argparse\n",
        "#from sklearn.ensemble import GradientBoostingClassifier\n",
        "#from sklearn.metrics import classification_report\n",
        "import os\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "#import statsmodels.api as sm\n",
        "#import statsmodels.stats.diagnostic as smd\n",
        "#import statsmodels.stats.stattools as sms\n",
        "#from statsmodels.compat import lzip\n",
        "#from statsmodels.tools.eval_measures import rmse\n",
        "from sklearn.metrics import r2_score,mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "def select_first_file(path):\n",
        "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
        "    Args:\n",
        "        path (str): path to directory or file to choose\n",
        "    Returns:\n",
        "        str: full path of selected file\n",
        "    \"\"\"\n",
        "    files = os.listdir(path)\n",
        "    return os.path.join(path, files[0])\n",
        "\n",
        "\n",
        "# Start Logging\n",
        "mlflow.start_run()\n",
        "\n",
        "# enable autologging\n",
        "mlflow.sklearn.autolog()\n",
        "\n",
        "os.makedirs(\"./outputs\", exist_ok=True)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "    #parser.add_argument(\"--n_estimators\", required=False, default=100, type=int)\n",
        "    #parser.add_argument(\"--learning_rate\", required=False, default=0.1, type=float)\n",
        "    parser.add_argument(\"--registered_model_name1\", type=str, help=\"model name\")\n",
        "    parser.add_argument(\"--model1\", type=str, help=\"path to model file\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
        "    train_data = pd.read_csv(select_first_file(args.train_data))\n",
        "\n",
        "    # Extracting the label column\n",
        "    y_train = train_data.pop(\"total_lift\")\n",
        "\n",
        "    # convert the dataframe values to array\n",
        "    X_train = train_data.values\n",
        "\n",
        "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
        "    test_data = pd.read_csv(select_first_file(args.test_data))\n",
        "\n",
        "    # Extracting the label column\n",
        "    y_test = test_data.pop(\"total_lift\")\n",
        "\n",
        "    # convert the dataframe values to array\n",
        "    X_test = test_data.values\n",
        "    \n",
        "    # Initialize the Linear Regression model\n",
        "    model_v1 = LinearRegression() \n",
        "    # Fit the model\n",
        "    model_v1.fit(X_train, y_train) \n",
        "    # Predict the training data\n",
        "    y_pred = model_v1.predict(X_train) \n",
        "    # Calculate Mean Squared Error\n",
        "    mse_v1 = mean_squared_error(y_train, y_pred, squared=False)\n",
        "\n",
        "    print(f\"Training with data of shape {X_train.shape}\")\n",
        "\n",
        "    #X_train = sm.add_constant(X_train)\n",
        "\n",
        "    #model = sm.OLS(y_train,X_train)\n",
        "    #results = model.fit(cov_type=\"HC0\")\n",
        "    #print(results.summary())\n",
        "\n",
        "    # Registering the model to the workspace\n",
        "    print(\"Registering the model via MLFlow\")\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=model_v1,\n",
        "        registered_model_name=args.registered_model_name1,\n",
        "        artifact_path=\"model_v1\",\n",
        "    )\n",
        "\n",
        "    # Saving the model to a file\n",
        "    mlflow.sklearn.save_model(\n",
        "        sk_model=model_v1,\n",
        "        path=os.path.join(args.model1, \"trained_model1\"),\n",
        "    )\n",
        "\n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/train/train.py\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "train.py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {train_src_dir}/train.yml\n",
        "# <component>\n",
        "name: train_model\n",
        "display_name: Train Model\n",
        "# version: 1 # Not specifying a version will automatically update the version\n",
        "type: command\n",
        "inputs:\n",
        "  train_data: \n",
        "    type: uri_folder\n",
        "  test_data: \n",
        "    type: uri_folder    \n",
        "  registered_model_name:\n",
        "    type: string\n",
        "outputs:\n",
        "  model:\n",
        "    type: uri_folder\n",
        "code: .\n",
        "environment:\n",
        "  # for this step, we'll use an AzureML curate environment\n",
        "  azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\n",
        "command: >\n",
        "  python train.py \n",
        "  --train_data ${{inputs.train_data}} \n",
        "  --test_data ${{inputs.test_data}} \n",
        "  --registered_model_name ${{inputs.registered_model_name}} \n",
        "  --model ${{outputs.model}}\n",
        "# </component>\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/train/train.yml\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "train.yml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the Component Package\n",
        "from azure.ai.ml import load_component\n",
        "\n",
        "# Loading the component from the yml file\n",
        "train_component = load_component(source=os.path.join(train_src_dir, \"train.yml\"))\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "train_component = ml_client.create_or_update(train_component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {train_component.name} with Version {train_component.version} is registered\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component train_model with Version 2023-11-01-19-57-03-8244820 is registered\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "train_component",
        "gather": {
          "logged": 1698868629905
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model v2 Component**\n",
        "\n",
        "**New Model: Gradient Boost Regressor**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "trainv2_src_dir = \"./components/trainv2\"\n",
        "os.makedirs(trainv2_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1698868630661
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {trainv2_src_dir}/trainv2.py\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import argparse\n",
        "import os\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def select_first_file(path):\n",
        "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
        "    Args:\n",
        "        path (str): path to directory or file to choose\n",
        "    Returns:\n",
        "        str: full path of selected file\n",
        "    \"\"\"\n",
        "    files = os.listdir(path)\n",
        "    return os.path.join(path, files[0])\n",
        "\n",
        "\n",
        "# Start Logging\n",
        "mlflow.start_run()\n",
        "\n",
        "# enable autologging\n",
        "mlflow.sklearn.autolog()\n",
        "\n",
        "os.makedirs(\"./outputs\", exist_ok=True)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "    parser.add_argument(\"--registered_model_name2\", type=str, help=\"model name\")\n",
        "    parser.add_argument(\"--model2\", type=str, help=\"path to model file\")\n",
        "    parser.add_argument(\"--n_estimators\", required=False, default=100, type=int)\n",
        "    parser.add_argument(\"--learning_rate\", required=False, default=0.1, type=float)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
        "    train_data = pd.read_csv(select_first_file(args.train_data))\n",
        "\n",
        "    # Extracting the label column\n",
        "    y_train = train_data.pop(\"total_lift\")\n",
        "\n",
        "    # convert the dataframe values to array\n",
        "    X_train = train_data.values\n",
        "\n",
        "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
        "    test_data = pd.read_csv(select_first_file(args.test_data))\n",
        "\n",
        "    # Extracting the label column\n",
        "    y_test = test_data.pop(\"total_lift\")\n",
        "\n",
        "    # convert the dataframe values to array\n",
        "    X_test = test_data.values\n",
        "\n",
        "    # Initialize the Gradient Boosting Regressor model\n",
        "    model_v2 = GradientBoostingRegressor(n_estimators=args.n_estimators, learning_rate=args.learning_rate)\n",
        "    # Fit the model\n",
        "    model_v2.fit(X_train, y_train)\n",
        "    # Predict the training data\n",
        "    y_pred = model_v2.predict(X_train)\n",
        "    # Calculate Mean Squared Error\n",
        "    mse_v2 = mean_squared_error(y_train, y_pred, squared=False)\n",
        "\n",
        "    print(f\"Training with data of shape {X_train.shape}\")\n",
        "\n",
        "    # Registering the model to the workspace\n",
        "    print(\"Registering the model via MLFlow\")\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=model_v2,\n",
        "        registered_model_name=args.registered_model_name2,\n",
        "        artifact_path=\"model_v2\",\n",
        "    )\n",
        "\n",
        "    # Saving the model to a file\n",
        "    mlflow.sklearn.save_model(\n",
        "        sk_model=model_v2,\n",
        "        path=os.path.join(args.model2, \"trained_model2\"),\n",
        "    )\n",
        "\n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/trainv2/trainv2.py\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1698850023346
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {trainv2_src_dir}/trainv2.yml\n",
        "# <component>\n",
        "name: train_modelv2\n",
        "display_name: Train Model v2\n",
        "# version: 1 # Not specifying a version will automatically update the version\n",
        "type: command\n",
        "inputs:\n",
        "  train_data: \n",
        "    type: uri_folder\n",
        "  test_data: \n",
        "    type: uri_folder    \n",
        "  registered_model_name:\n",
        "    type: string\n",
        "outputs:\n",
        "  model:\n",
        "    type: uri_folder\n",
        "code: .\n",
        "environment:\n",
        "  # for this step, we'll use an AzureML curate environment\n",
        "  azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\n",
        "command: >\n",
        "  python trainv2.py \n",
        "  --train_data ${{inputs.train_data}} \n",
        "  --test_data ${{inputs.test_data}} \n",
        "  --registered_model_name ${{inputs.registered_model_name}} \n",
        "  --model ${{outputs.model}}\n",
        "# </component>\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/trainv2/trainv2.yml\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the component from the yml file\n",
        "train_component2 = load_component(source=os.path.join(trainv2_src_dir, \"trainv2.yml\"))\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "train_component2 = ml_client.create_or_update(train_component2)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {train_component2.name} with Version {train_component2.version} is registered\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component train_modelv2 with Version 2023-11-01-19-57-05-9035667 is registered\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1698868633324
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model v3: Gradient Boost Regressor w/ Altered Hyperparameters**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "trainv3_src_dir = \"./components/trainv3\"\n",
        "os.makedirs(trainv3_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1698868634020
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {trainv3_src_dir}/trainv3.py\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import argparse\n",
        "import os\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def select_first_file(path):\n",
        "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
        "    Args:\n",
        "        path (str): path to directory or file to choose\n",
        "    Returns:\n",
        "        str: full path of selected file\n",
        "    \"\"\"\n",
        "    files = os.listdir(path)\n",
        "    return os.path.join(path, files[0])\n",
        "\n",
        "\n",
        "# Start Logging\n",
        "mlflow.start_run()\n",
        "\n",
        "# enable autologging\n",
        "mlflow.sklearn.autolog()\n",
        "\n",
        "os.makedirs(\"./outputs\", exist_ok=True)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "    parser.add_argument(\"--registered_model_name3\", type=str, help=\"model name\")\n",
        "    parser.add_argument(\"--model3\", type=str, help=\"path to model file\")\n",
        "    parser.add_argument(\"--n_estimators\", required=False, default=200, type=int)\n",
        "    parser.add_argument(\"--learning_rate\", required=False, default=0.05, type=float)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
        "    train_data = pd.read_csv(select_first_file(args.train_data))\n",
        "\n",
        "    # Extracting the label column\n",
        "    y_train = train_data.pop(\"total_lift\")\n",
        "\n",
        "    # convert the dataframe values to array\n",
        "    X_train = train_data.values\n",
        "\n",
        "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
        "    test_data = pd.read_csv(select_first_file(args.test_data))\n",
        "\n",
        "    # Extracting the label column\n",
        "    y_test = test_data.pop(\"total_lift\")\n",
        "\n",
        "    # convert the dataframe values to array\n",
        "    X_test = test_data.values\n",
        "\n",
        "    # Initialize the Gradient Boosting Regressor model\n",
        "    model_v3 = GradientBoostingRegressor(n_estimators=args.n_estimators, learning_rate=args.learning_rate)\n",
        "    # Fit the model\n",
        "    model_v3.fit(X_train, y_train)\n",
        "    # Predict the training data\n",
        "    y_pred = model_v3.predict(X_train)\n",
        "    # Calculate Mean Squared Error\n",
        "    mse_v3 = mean_squared_error(y_train, y_pred, squared=False)\n",
        "\n",
        "    print(f\"Training with data of shape {X_train.shape}\")\n",
        "\n",
        "    # Registering the model to the workspace\n",
        "    print(\"Registering the model via MLFlow\")\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=model_v3,\n",
        "        registered_model_name=args.registered_model_name3,\n",
        "        artifact_path=\"model_v3\",\n",
        "    )\n",
        "\n",
        "    # Saving the model to a file\n",
        "    mlflow.sklearn.save_model(\n",
        "        sk_model=model_v3,\n",
        "        path=os.path.join(args.model3, \"trained_model3\"),\n",
        "    )\n",
        "\n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/trainv3/trainv3.py\n"
        }
      ],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {trainv3_src_dir}/trainv3.yml\n",
        "# <component>\n",
        "name: train_modelv3\n",
        "display_name: Train Model v3\n",
        "# version: 1 # Not specifying a version will automatically update the version\n",
        "type: command\n",
        "inputs:\n",
        "  train_data: \n",
        "    type: uri_folder\n",
        "  test_data: \n",
        "    type: uri_folder    \n",
        "  registered_model_name:\n",
        "    type: string\n",
        "outputs:\n",
        "  model:\n",
        "    type: uri_folder\n",
        "code: .\n",
        "environment:\n",
        "  # for this step, we'll use an AzureML curate environment\n",
        "  azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\n",
        "command: >\n",
        "  python trainv3.py \n",
        "  --train_data ${{inputs.train_data}} \n",
        "  --test_data ${{inputs.test_data}} \n",
        "  --registered_model_name ${{inputs.registered_model_name}} \n",
        "  --model ${{outputs.model}}\n",
        "# </component>\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/trainv3/trainv3.yml\n"
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the component from the yml file\n",
        "train_component3 = load_component(source=os.path.join(trainv3_src_dir, \"trainv3.yml\"))\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "train_component3 = ml_client.create_or_update(train_component3)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {train_component3.name} with Version {train_component3.version} is registered\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component train_modelv3 with Version 2023-11-01-19-57-07-8823370 is registered\n"
        }
      ],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1698868636714
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "**Pipeline Creation**\n",
        "\n",
        "**Experiment 1**\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=cpu_compute_target\n",
        "    if (cpu_cluster)\n",
        "    else \"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n",
        "    description=\"Experiment 1 with Cleaned Data and Original Linear Regression Model\",\n",
        ")\n",
        "def pipeline(\n",
        "    pipeline_job_data_input,\n",
        "    pipeline_job_test_train_ratio,\n",
        "    pipeline_job_learning_rate,\n",
        "    pipeline_job_registered_model_name,\n",
        "):\n",
        "    # using data_prep_function like a python call with its own inputs\n",
        "    data_prep_job = data_prep_component(\n",
        "        data=pipeline_job_data_input,\n",
        "        test_train_ratio=pipeline_job_test_train_ratio,\n",
        "    )\n",
        "\n",
        "    # using train_func like a python call with its own inputs\n",
        "    train_job = train_component(\n",
        "        train_data=data_prep_job.outputs.train_data,  # note: using outputs from previous step\n",
        "        test_data=data_prep_job.outputs.test_data,  # note: using outputs from previous step\n",
        "        registered_model_name=pipeline_job_registered_model_name,\n",
        "    )\n",
        "\n",
        "    # a pipeline returns a dictionary of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        \"pipeline_job_train_data\": data_prep_job.outputs.train_data,\n",
        "        \"pipeline_job_test_data\": data_prep_job.outputs.test_data,\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "pipeline",
        "gather": {
          "logged": 1698868637729
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "registered_model_name = \"model1\"\n",
        "\n",
        "# Let's instantiate the pipeline with the parameters of our choice\n",
        "pipeline = pipeline(\n",
        "    pipeline_job_data_input=Input(type=\"uri_file\", path=data_asset.path),\n",
        "    pipeline_job_test_train_ratio=0.2,\n",
        "    pipeline_job_registered_model_name=registered_model_name,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "registered_model_name",
        "gather": {
          "logged": 1698868638595
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline,\n",
        "    # Project's name\n",
        "    experiment_name=\"Experiment 1\",\n",
        ")\n",
        "ml_client.jobs.stream(pipeline_job.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nRequest time out. Ingestion may be backed up. Retrying.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RunId: eager_wire_60q4zhc5l3\nWeb View: https://ml.azure.com/runs/eager_wire_60q4zhc5l3?wsid=/subscriptions/5405ca59-d0a7-4e18-af16-b7f21ebcb1b0/resourcegroups/mlops_group6/workspaces/mlops_assignment2\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2023-11-01 19:57:57Z] Submitting 1 runs, first five are: ac18da2f:141ad396-67cb-4b84-bebf-0c5d6497e97f\n[2023-11-01 19:59:23Z] Completing processing run id 141ad396-67cb-4b84-bebf-0c5d6497e97f.\n[2023-11-01 19:59:24Z] Submitting 1 runs, first five are: 7e144ff0:40f216ae-db5d-4f4a-b9be-182fd37bf75a\n[2023-11-01 20:01:30Z] Completing processing run id 40f216ae-db5d-4f4a-b9be-182fd37bf75a.\n\nExecution Summary\n=================\nRunId: eager_wire_60q4zhc5l3\nWeb View: https://ml.azure.com/runs/eager_wire_60q4zhc5l3?wsid=/subscriptions/5405ca59-d0a7-4e18-af16-b7f21ebcb1b0/resourcegroups/mlops_group6/workspaces/mlops_assignment2\n\n"
        }
      ],
      "execution_count": 29,
      "metadata": {
        "name": "returned_job",
        "gather": {
          "logged": 1698868936822
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating new versions of the pipeline to run different experiments**\n",
        "\n",
        "**Experiment 2: Data Version 2**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=cpu_compute_target\n",
        "    if (cpu_cluster)\n",
        "    else \"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n",
        "    description=\"Experiment V2 with DataV2\",\n",
        ")\n",
        "def pipeline(\n",
        "    pipeline_job_data_input,\n",
        "    pipeline_job_test_train_ratio,\n",
        "    pipeline_job_learning_rate,\n",
        "    pipeline_job_registered_model_name,\n",
        "):\n",
        "    # using data_version_function like a python call with its own inputs\n",
        "    data_version_job = data_version_component(\n",
        "        data=pipeline_job_data_input,\n",
        "        test_train_ratio=pipeline_job_test_train_ratio,\n",
        "    )\n",
        "\n",
        "    # using train_func like a python call with its own inputs\n",
        "    train_job = train_component(\n",
        "        train_data=data_version_job.outputs.train_data,  # note: using outputs from previous step\n",
        "        test_data=data_version_job.outputs.test_data,  # note: using outputs from previous step\n",
        "        registered_model_name=pipeline_job_registered_model_name,\n",
        "    )\n",
        "\n",
        "    # a pipeline returns a dictionary of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        \"pipeline_job_train_data\": data_version_job.outputs.train_data,\n",
        "        \"pipeline_job_test_data\": data_version_job.outputs.test_data,\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1698860598767
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "registered_model_name = \"model2\"\n",
        "\n",
        "# Let's instantiate the pipeline with the parameters of our choice\n",
        "pipeline = pipeline(\n",
        "    pipeline_job_data_input=Input(type=\"uri_file\", path=data_asset.path),\n",
        "    pipeline_job_test_train_ratio=0.2,\n",
        "    pipeline_job_registered_model_name=registered_model_name,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 45,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1698860599141
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline,\n",
        "    # Project's name\n",
        "    experiment_name=\"Experiment 2 With Data V2\",\n",
        ")\n",
        "ml_client.jobs.stream(pipeline_job.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RunId: jovial_airport_3n8g4nzj93\nWeb View: https://ml.azure.com/runs/jovial_airport_3n8g4nzj93?wsid=/subscriptions/5405ca59-d0a7-4e18-af16-b7f21ebcb1b0/resourcegroups/mlops_group6/workspaces/mlops_assignment2\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2023-11-01 17:43:24Z] Completing processing run id f4486d2e-e620-40d9-a236-43f7d76fa30b.\n[2023-11-01 17:43:25Z] Submitting 1 runs, first five are: 770a2ccb:c1fc51fb-c1c5-4f3d-9036-31f39e57e2eb\n[2023-11-01 17:44:22Z] Completing processing run id c1fc51fb-c1c5-4f3d-9036-31f39e57e2eb.\n\nExecution Summary\n=================\nRunId: jovial_airport_3n8g4nzj93\nWeb View: https://ml.azure.com/runs/jovial_airport_3n8g4nzj93?wsid=/subscriptions/5405ca59-d0a7-4e18-af16-b7f21ebcb1b0/resourcegroups/mlops_group6/workspaces/mlops_assignment2\n\n"
        }
      ],
      "execution_count": 46,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1698860671317
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment V3 with Data Version V2 and Model V2**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=cpu_compute_target\n",
        "    if (cpu_cluster)\n",
        "    else \"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n",
        "    description=\"Experiment V3 with DataV2 and Model V2\",\n",
        ")\n",
        "def pipeline(\n",
        "    pipeline_job_data_input,\n",
        "    pipeline_job_test_train_ratio,\n",
        "    pipeline_job_learning_rate,\n",
        "    pipeline_job_registered_model_name,\n",
        "):\n",
        "    # using data_version_function like a python call with its own inputs\n",
        "    data_version_job = data_version_component(\n",
        "        data=pipeline_job_data_input,\n",
        "        test_train_ratio=pipeline_job_test_train_ratio,\n",
        "    )\n",
        "\n",
        "    # using train_func like a python call with its own inputs\n",
        "    train_job = train_component2(\n",
        "        train_data=data_version_job.outputs.train_data,  # note: using outputs from previous step\n",
        "        test_data=data_version_job.outputs.test_data,  # note: using outputs from previous step\n",
        "        registered_model_name=pipeline_job_registered_model_name,\n",
        "    )\n",
        "\n",
        "    # a pipeline returns a dictionary of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        \"pipeline_job_train_data\": data_version_job.outputs.train_data,\n",
        "        \"pipeline_job_test_data\": data_version_job.outputs.test_data,\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": 47,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1698860671597
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "registered_model_name = \"model3\"\n",
        "\n",
        "# Let's instantiate the pipeline with the parameters of our choice\n",
        "pipeline = pipeline(\n",
        "    pipeline_job_data_input=Input(type=\"uri_file\", path=data_asset.path),\n",
        "    pipeline_job_test_train_ratio=0.2,\n",
        "    pipeline_job_registered_model_name=registered_model_name,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 48,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1698860671823
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline,\n",
        "    # Project's name\n",
        "    experiment_name=\"Experiment 3 With Data V2 and Model V2\",\n",
        ")\n",
        "ml_client.jobs.stream(pipeline_job.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RunId: quirky_cart_x4m2j217lp\nWeb View: https://ml.azure.com/runs/quirky_cart_x4m2j217lp?wsid=/subscriptions/5405ca59-d0a7-4e18-af16-b7f21ebcb1b0/resourcegroups/mlops_group6/workspaces/mlops_assignment2\n[2023-11-01 17:45:29Z] Completing processing run id f6f46854-717b-486f-bbec-a94af72747e1.\n\nExecution Summary\n=================\nRunId: quirky_cart_x4m2j217lp\nWeb View: https://ml.azure.com/runs/quirky_cart_x4m2j217lp?wsid=/subscriptions/5405ca59-d0a7-4e18-af16-b7f21ebcb1b0/resourcegroups/mlops_group6/workspaces/mlops_assignment2\n\n"
        }
      ],
      "execution_count": 49,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1698860733829
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment V4 With Data Version V2 and Model V2 with different hyperparameters**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=cpu_compute_target\n",
        "    if (cpu_cluster)\n",
        "    else \"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n",
        "    description=\"Experiment V4 with DataV2, Model V2, and Altered Hyperparameters\",\n",
        ")\n",
        "def pipeline(\n",
        "    pipeline_job_data_input,\n",
        "    pipeline_job_test_train_ratio,\n",
        "    pipeline_job_learning_rate,\n",
        "    pipeline_job_registered_model_name,\n",
        "):\n",
        "    # using data_version_function like a python call with its own inputs\n",
        "    data_version_job = data_version_component(\n",
        "        data=pipeline_job_data_input,\n",
        "        test_train_ratio=pipeline_job_test_train_ratio,\n",
        "    )\n",
        "\n",
        "    # using train_func like a python call with its own inputs\n",
        "    train_job = train_component3(\n",
        "        train_data=data_version_job.outputs.train_data,  # note: using outputs from previous step\n",
        "        test_data=data_version_job.outputs.test_data,  # note: using outputs from previous step\n",
        "        registered_model_name=pipeline_job_registered_model_name,\n",
        "    )\n",
        "\n",
        "    # a pipeline returns a dictionary of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        \"pipeline_job_train_data\": data_version_job.outputs.train_data,\n",
        "        \"pipeline_job_test_data\": data_version_job.outputs.test_data,\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": 50,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1698860734062
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "registered_model_name = \"model4\"\n",
        "\n",
        "# Let's instantiate the pipeline with the parameters of our choice\n",
        "pipeline = pipeline(\n",
        "    pipeline_job_data_input=Input(type=\"uri_file\", path=data_asset.path),\n",
        "    pipeline_job_test_train_ratio=0.2,\n",
        "    pipeline_job_registered_model_name=registered_model_name,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 51,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1698860734302
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline,\n",
        "    # Project's name\n",
        "    experiment_name=\"Experiment 4 With Data V2, Model V2, and Altered Hyperparameters\",\n",
        ")\n",
        "ml_client.jobs.stream(pipeline_job.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RunId: coral_nail_qm23phc3hq\nWeb View: https://ml.azure.com/runs/coral_nail_qm23phc3hq?wsid=/subscriptions/5405ca59-d0a7-4e18-af16-b7f21ebcb1b0/resourcegroups/mlops_group6/workspaces/mlops_assignment2\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2023-11-01 17:45:38Z] Completing processing run id 36da371e-fa9f-41b1-8236-62774d308091.\n[2023-11-01 17:45:39Z] Submitting 1 runs, first five are: 9491b583:79c5f2f5-0d14-48a9-a24c-fd263276165e\n[2023-11-01 17:46:33Z] Completing processing run id 79c5f2f5-0d14-48a9-a24c-fd263276165e.\n\nExecution Summary\n=================\nRunId: coral_nail_qm23phc3hq\nWeb View: https://ml.azure.com/runs/coral_nail_qm23phc3hq?wsid=/subscriptions/5405ca59-d0a7-4e18-af16-b7f21ebcb1b0/resourcegroups/mlops_group6/workspaces/mlops_assignment2\n\n"
        }
      ],
      "execution_count": 52,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1698860795289
        }
      }
    }
  ],
  "metadata": {
    "description": {
      "description": "Create production ML pipelines with Python SDK v2 in a Jupyter notebook"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "categories": [
      "SDK v2",
      "tutorials",
      "get-started-notebooks"
    ],
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}